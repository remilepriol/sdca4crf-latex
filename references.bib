@article{wilson_lyapunov_2016,
	title = {A {Lyapunov} {Analysis} of {Momentum} {Methods} in {Optimization}},
	url = {http://arxiv.org/abs/1611.02635},
	abstract = {Momentum methods play a central role in optimization. Several momentum methods are provably optimal, and all use a technique called estimate sequences to analyze their convergence properties. The technique of estimate sequences has long been considered difficult to understand, leading many researchers to generate alternative, "more intuitive" methods and analyses. In this paper we show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This framework allows us to develop a simple and unified analysis of many existing momentum algorithms, introduce several new algorithms, and most importantly, strengthen the connection between algorithms and continuous-time dynamical systems.},
	journal = {arXiv:1611.02635 [cs, math]},
	author = {Wilson, Ashia C. and Recht, Benjamin and Jordan, Michael I.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02635},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Optimization and Control},
	file = {arXiv\:1611.02635 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/43KAQH7X/Wilson et al. - 2016 - A Lyapunov Analysis of Momentum Methods in Optimiz.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/P8J6C4RX/1611.html:text/html}
}

@book{kakade_duality_2009,
	title = {On the duality of strong convexity and strong smoothness: {Learning} applications and matrix regularization},
	shorttitle = {On the duality of strong convexity and strong smoothness},
	abstract = {We show that a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. Utilizing this duality, we isolate a single inequality which seamlessly implies both generalization bounds and online regret bounds; and we show how to construct strongly convex functions over matrices based on strongly convex functions over vectors. The newly constructed functions (over matrices) inherit the strong convexity properties of the underlying vector functions. We demonstrate the potential of this framework by analyzing several learning algorithms including group Lasso, kernel learning, and online control with adversarial quadratic costs. 1},
	author = {Kakade, Sham M. and Shalev-shwartz, Shai and Tewari, Ambuj},
	year = {2009},
	file = {Citeseer - Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/5BQ7977A/Kakade et al. - On the duality of strong convexity and strong smoo.pdf:application/pdf;Citeseer - Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/83TMZT3P/summary.html:text/html}
}

@incollection{shalev-shwartz_accelerated_2013,
	title = {Accelerated {Mini}-{Batch} {Stochastic} {Dual} {Coordinate} {Ascent}},
	url = {http://papers.nips.cc/paper/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {378--385},
	file = {NIPS Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/XWD5WRR5/Shalev-Shwartz and Zhang - 2013 - Accelerated Mini-Batch Stochastic Dual Coordinate .pdf:application/pdf;NIPS Snapshort:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/M2CWVRA2/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.html:text/html}
}

@article{shalev-shwartz_stochastic_2013,
	title = {Stochastic {Dual} {Coordinate} {Ascent} {Methods} for {Regularized} {Loss} {Minimization}},
	volume = {14},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v14/shalev-shwartz13a.html},
	number = {Feb},
	urldate = {2017-07-06},
	journal = {Journal of Machine Learning Research},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	year = {2013},
	pages = {567--599},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/RC7UTQAC/Shalev-Shwartz and Zhang - 2013 - Stochastic Dual Coordinate Ascent Methods for Regu.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/MEQA7X78/shalev-shwartz13a.html:text/html}
}

@inproceedings{shalev-shwartz_accelerated_2014,
	title = {Accelerated {Proximal} {Stochastic} {Dual} {Coordinate} {Ascent} for {Regularized} {Loss} {Minimization}},
	url = {http://proceedings.mlr.press/v32/shalev-shwartz14.html},
	abstract = {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framewo...},
	language = {en},
	urldate = {2017-07-06},
	booktitle = {{PMLR}},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = jan,
	year = {2014},
	pages = {64--72},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/PSNNVXKH/Shalev-Shwartz and Zhang - 2014 - Accelerated Proximal Stochastic Dual Coordinate As.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/3V5KQBBE/shalev-shwartz14.html:text/html}
}

@article{shalev-shwartz_accelerated_2013-1,
	title = {Accelerated {Proximal} {Stochastic} {Dual} {Coordinate} {Ascent} for {Regularized} {Loss} {Minimization}},
	url = {http://arxiv.org/abs/1309.2375},
	abstract = {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.},
	journal = {arXiv:1309.2375 [cs, stat]},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.2375},
	keywords = {Computer Science - Learning, Computer Science - Numerical Analysis, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1309.2375 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/CRWBN39H/Shalev-Shwartz and Zhang - 2013 - Accelerated Proximal Stochastic Dual Coordinate As.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/MZE5MRKF/1309.html:text/html}
}

@inproceedings{csiba2015stochastic,
  title={Stochastic dual coordinate ascent with adaptive probabilities},
  author={Csiba, Dominik and Qu, Zheng and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  year={2015}
}

@inproceedings{shalev-shwartz_sdca_2016,
	title = {{SDCA} without {Duality}, {Regularization}, and {Individual} {Convexity}},
	url = {http://proceedings.mlr.press/v48/shalev-shwartza16.html},
	abstract = {Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularizati...},
	language = {en},
	urldate = {2017-07-27},
	booktitle = {{PMLR}},
	author = {Shalev-Shwartz, Shai},
	month = jun,
	year = {2016},
	pages = {747--754},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/WPDNWX5I/Shalev-Shwartz - 2016 - SDCA without Duality, Regularization, and Individu.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/AGDJGW2A/shalev-shwartza16.html:text/html}
}

@article{nesterov_efficiency_2012,
	title = {Efficiency of coordinate descent methods on huge-scale optimization problems},
	volume = {22},
	url = {http://epubs.siam.org/doi/abs/10.1137/100802001},
	number = {2},
	journal = {SIAM Journal on Optimization},
	author = {Nesterov, Yurii},
	year = {2012},
	pages = {341--362},
	file = {[PDF] from siam.org:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/S6CU94QN/Nesterov - 2012 - Efficiency of coordinate descent methods on huge-s.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2929H9K2/100802001.html:text/html}
}

@article{shalev-shwartz_online_2007,
	title = {Online {Learning}: {Theory}, {Algorithms}, and {Applications}},
	url = {http://w3.cs.huji.ac.il/~shais/papers/ShalevThesis07.pdf},
	urldate = {2017-07-28},
	author = {Shalev-Shwartz, Shai},
	year = {2007},
	file = {ShalevThesis07.pdf:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2VKVBC4K/ShalevThesis07.pdf:application/pdf}
}

@article{muller_pystruct_2014,
	title = {pystruct - {Learning} {Structured} {Prediction} in {Python}},
	volume = {15},
	url = {http://www.jmlr.org/papers/v15/mueller14a.html},
	urldate = {2017-07-31},
	journal = {Journal of Machine Learning Research},
	author = {Müller, Andreas C. and Behnke, Sven},
	year = {2014},
	pages = {2055--2060},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/JUJ8SEJW/Müller and Behnke - 2014 - pystruct - Learning Structured Prediction in Pytho.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/T4PWIT3T/mueller14a.html:text/html}
}

@article{lacoste-julien_block-coordinate_2012,
	title = {Block-{Coordinate} {Frank}-{Wolfe} {Optimization} for {Structural} {SVMs}},
	url = {http://arxiv.org/abs/1207.4747},
	abstract = {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.},
	journal = {arXiv:1207.4747 [cs, math, stat]},
	author = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.4747},
	keywords = {90C52, 90C90, 90C06, 68T05, Computer Science - Learning, G.1.6, I.2.6, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv\:1207.4747 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/PJNSV8CJ/Lacoste-Julien et al. - 2012 - Block-Coordinate Frank-Wolfe Optimization for Stru.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/4HIMVM6I/1207.html:text/html}
}

@inproceedings{osokin2016minding,
  title={Minding the gaps for block {Frank-Wolfe} optimization of structured {SVM}s},
  author={Osokin, Anton and Alayrac, Jean-Baptiste and Lukasewitz, Isabella and Dokania, Puneet and Lacoste-Julien, Simon},
  booktitle={International Conference on Machine Learning},
  year={2016}
}

@book{press_numerical_1992,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Numerical recipes in {C}: the art of scientific computing},
	isbn = {978-0-521-43108-8 978-0-521-43720-2},
	shorttitle = {Numerical recipes in {C}},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	year = {1992},
	keywords = {C (Computer program language)},
	file = {Numerical_Recipes.pdf:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/HUDG92EK/Numerical_Recipes.pdf:application/pdf}
}

@article{dziugaite_computing_2017,
	title = {Computing {Nonvacuous} {Generalization} {Bounds} for {Deep} ({Stochastic}) {Neural} {Networks} with {Many} {More} {Parameters} than {Training} {Data}},
	url = {http://arxiv.org/abs/1703.11008},
	abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous in this "deep learning" regime. In order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
	journal = {arXiv:1703.11008 [cs]},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.11008},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1703.11008 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/28H3KVZP/Dziugaite and Roy - 2017 - Computing Nonvacuous Generalization Bounds for Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/F74D7BV6/1703.html:text/html}
}

@article{collins2008exponentiated,
  title={Exponentiated gradient algorithms for conditional random fields and max-margin {M}arkov networks},
  author={Collins, Michael and Globerson, Amir and Koo, Terry and Carreras, Xavier and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  year={2008}
}

@inproceedings{zhao_stochastic_2015,
	title = {Stochastic {Optimization} with {Importance} {Sampling} for {Regularized} {Loss} {Minimization}},
	url = {http://proceedings.mlr.press/v37/zhaoa15.html},
	abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordin...},
	language = {en},
	urldate = {2017-08-29},
	booktitle = {{PMLR}},
	author = {Zhao, Peilin and Zhang, Tong},
	month = jun,
	year = {2015},
	pages = {1--9},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/VX3T7AFQ/Zhao and Zhang - 2015 - Stochastic Optimization with Importance Sampling f.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/AD3CE44P/zhaoa15.html:text/html}
}

@inproceedings{schmidt2015non,
  title={Non-uniform stochastic average gradient method for training conditional random fields},
  author={Schmidt, Mark and Babanezhad, Reza and Ahmed, Mohamed and Defazio, Aaron and Clifton, Ann and Sarkar, Anoop},
  booktitle={Artificial Intelligence and Statistics},
  year={2015}
}

@article{taskar_structured_2006,
	title = {Structured {Prediction}, {Dual} {Extragradient} and {Bregman} {Projections}},
	volume = {7},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v7/taskar06a.html},
	number = {Jul},
	urldate = {2017-09-11},
	journal = {Journal of Machine Learning Research},
	author = {Taskar, Ben and Lacoste-Julien, Simon and Jordan, Michael I.},
	year = {2006},
	pages = {1627--1653},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/JRUDXTCX/Taskar et al. - 2006 - Structured Prediction, Dual Extragradient and Breg.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/7N2AX5T2/taskar06a.html:text/html}
}

@inproceedings{defazio2014saga,
  title={{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2014}
}

@inproceedings{jaggi_revisiting_2013,
	title = {Revisiting {Frank}-{Wolfe}: {Projection}-{Free} {Sparse} {Convex} {Optimization}},
	shorttitle = {Revisiting {Frank}-{Wolfe}},
	url = {http://proceedings.mlr.press/v28/jaggi13.html},
	abstract = {We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework...},
	language = {en},
	urldate = {2017-09-11},
	booktitle = {{PMLR}},
	author = {Jaggi, Martin},
	month = feb,
	year = {2013},
	pages = {427--435},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/3ZF746RM/Jaggi - 2013 - Revisiting Frank-Wolfe Projection-Free Sparse Con.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/VRQGDUQX/jaggi13.html:text/html}
}

@inproceedings{taskar2004max,
  title={Max-margin {M}arkov networks},
  author={Taskar, Ben and Guestrin, Carlos and Koller, Daphne},
  booktitle={Advances in Neural Information Processing Systems},
  year={2004}
}

@inproceedings{lafferty2001conditional,
  title={Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  author={Lafferty, John and McCallum, Andrew and Pereira, Fernando CN},
  booktitle={International Conference on Machine Learning},
  year={2001}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  year={2017},
  publisher={Springer}
}

@inproceedings{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence rate for finite training sets},
  author={Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}

@MastersThesis{wallach2002efficient,
  title={Efficient training of conditional random fields},
  author={Wallach, Hanna},
  year={2002},
  school={University of Edinburgh}
}

@inproceedings{sha2003shallow,
  title={Shallow parsing with conditional random fields},
  author={Sha, Fei and Pereira, Fernando},
  booktitle={Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology},
  year={2003}
}

@inproceedings{vishwanathan_accelerated_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Accelerated {Training} of {Conditional} {Random} {Fields} with {Stochastic} {Gradient} {Methods}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143966},
	doi = {10.1145/1143844.1143966},
	abstract = {We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Vishwanathan, S. V. N. and Schraudolph, Nicol N. and Schmidt, Mark W. and Murphy, Kevin P.},
	year = {2006},
	pages = {969--976}
}

@inproceedings{finkel_efficient_2008,
	title = {Efficient, {Feature}-based, {Conditional} {Random} {Field} {Parsing}},
	url = {http://aclasb.dfki.de/nlp/bib/P08-1109},
	urldate = {2017-09-14},
	publisher = {Association for Computational Linguistics},
	author = {Finkel, Jenny Rose and Kleeman, Alex and Manning, Christopher D.},
	year = {2008},
	pages = {959--967},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/4J4SNNJM/Finkel et al. - 2008 - Efficient, Feature-based, Conditional Random Field.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/UNGCNWAW/P08-1109.html:text/html}
}

@book{noauthor_introductory_nodate,
	title = {Introductory {Lectures} on {Convex} {Optimization} - {A} {Basic} {Course} {\textbar} {Yurii} {Nesterov} {\textbar} {Springer}},
	url = {http://www.springer.com/gp/book/9781402075537},
	abstract = {It was in the middle of the 1980s, when the seminal paper by Kar­ markar opened a new epoch in nonlinear optimization. The importance of this paper,...},
	urldate = {2017-09-14},
	file = {Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/EIHWKFP8/9781402075537.html:text/html}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  year={2013}
}

@book{blackard_comparative_1999,
	title = {Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables},
	author = {Blackard, Jock A. and Dean, Denis J.},
	year = {1999},
	file = {Citeseer - Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/6K7FVNNI/summary.html:text/html}
}

@inproceedings{lebanon2002boosting,
  title={Boosting and maximum likelihood for exponential models},
  author={Lebanon, Guy and Lafferty, John D},
  booktitle={Advances in Neural Information Processing Systems},
  year={2002}
}
@inproceedings{lacoste2013block,
  title={Block-Coordinate {Frank-Wolfe} Optimization for Structural {SVM}s},
  author={Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
  booktitle={International Conference on Machine Learning},
  year={2013}
}

@article{shalev2016accelerated,
  title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Mathematical Programming},
  year={2016},
  publisher={Springer-Verlag New York, Inc.}
}

@inproceedings{kirshnan15barrierFW,
  title={Barrier {F}rank-{W}olfe for marginal inference},
  author={Rahul G. Krishnan and Simon Lacoste-Julien and David Sontag},
  booktitle={Advances in Neural Information Processing Systems},
  year={2015}
}
