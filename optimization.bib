
@article{wilson_lyapunov_2016,
	title = {A {Lyapunov} {Analysis} of {Momentum} {Methods} in {Optimization}},
	url = {http://arxiv.org/abs/1611.02635},
	abstract = {Momentum methods play a central role in optimization. Several momentum methods are provably optimal, and all use a technique called estimate sequences to analyze their convergence properties. The technique of estimate sequences has long been considered difficult to understand, leading many researchers to generate alternative, "more intuitive" methods and analyses. In this paper we show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This framework allows us to develop a simple and unified analysis of many existing momentum algorithms, introduce several new algorithms, and most importantly, strengthen the connection between algorithms and continuous-time dynamical systems.},
	journal = {arXiv:1611.02635 [cs, math]},
	author = {Wilson, Ashia C. and Recht, Benjamin and Jordan, Michael I.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02635},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Optimization and Control},
	file = {arXiv\:1611.02635 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/43KAQH7X/Wilson et al. - 2016 - A Lyapunov Analysis of Momentum Methods in Optimiz.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/P8J6C4RX/1611.html:text/html}
}

@book{kakade_duality_nodate,
	title = {On the duality of strong convexity and strong smoothness: {Learning} applications and matrix regularization},
	shorttitle = {On the duality of strong convexity and strong smoothness},
	abstract = {We show that a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. Utilizing this duality, we isolate a single inequality which seamlessly implies both generalization bounds and online regret bounds; and we show how to construct strongly convex functions over matrices based on strongly convex functions over vectors. The newly constructed functions (over matrices) inherit the strong convexity properties of the underlying vector functions. We demonstrate the potential of this framework by analyzing several learning algorithms including group Lasso, kernel learning, and online control with adversarial quadratic costs. 1},
	author = {Kakade, Sham M. and Shalev-shwartz, Shai and Tewari, Ambuj},
	file = {Citeseer - Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/5BQ7977A/Kakade et al. - On the duality of strong convexity and strong smoo.pdf:application/pdf;Citeseer - Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/83TMZT3P/summary.html:text/html}
}

@incollection{shalev-shwartz_accelerated_2013,
	title = {Accelerated {Mini}-{Batch} {Stochastic} {Dual} {Coordinate} {Ascent}},
	url = {http://papers.nips.cc/paper/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {378--385},
	file = {NIPS Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/XWD5WRR5/Shalev-Shwartz and Zhang - 2013 - Accelerated Mini-Batch Stochastic Dual Coordinate .pdf:application/pdf;NIPS Snapshort:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/M2CWVRA2/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.html:text/html}
}

@article{shalev-shwartz_stochastic_2013,
	title = {Stochastic {Dual} {Coordinate} {Ascent} {Methods} for {Regularized} {Loss} {Minimization}},
	volume = {14},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v14/shalev-shwartz13a.html},
	number = {Feb},
	urldate = {2017-07-06},
	journal = {Journal of Machine Learning Research},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	year = {2013},
	pages = {567--599},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/RC7UTQAC/Shalev-Shwartz and Zhang - 2013 - Stochastic Dual Coordinate Ascent Methods for Regu.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/MEQA7X78/shalev-shwartz13a.html:text/html}
}

@inproceedings{shalev-shwartz_accelerated_2014,
	title = {Accelerated {Proximal} {Stochastic} {Dual} {Coordinate} {Ascent} for {Regularized} {Loss} {Minimization}},
	url = {http://proceedings.mlr.press/v32/shalev-shwartz14.html},
	abstract = {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framewo...},
	language = {en},
	urldate = {2017-07-06},
	booktitle = {{PMLR}},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = jan,
	year = {2014},
	pages = {64--72},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/PSNNVXKH/Shalev-Shwartz and Zhang - 2014 - Accelerated Proximal Stochastic Dual Coordinate As.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/3V5KQBBE/shalev-shwartz14.html:text/html}
}

@article{shalev-shwartz_accelerated_2013-1,
	title = {Accelerated {Proximal} {Stochastic} {Dual} {Coordinate} {Ascent} for {Regularized} {Loss} {Minimization}},
	url = {http://arxiv.org/abs/1309.2375},
	abstract = {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.},
	journal = {arXiv:1309.2375 [cs, stat]},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.2375},
	keywords = {Computer Science - Learning, Computer Science - Numerical Analysis, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1309.2375 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/CRWBN39H/Shalev-Shwartz and Zhang - 2013 - Accelerated Proximal Stochastic Dual Coordinate As.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/MZE5MRKF/1309.html:text/html}
}

@inproceedings{csiba_stochastic_2015,
	title = {Stochastic {Dual} {Coordinate} {Ascent} with {Adaptive} {Probabilities}},
	url = {http://proceedings.mlr.press/v37/csiba15.html},
	abstract = {This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowin...},
	language = {en},
	urldate = {2017-07-27},
	booktitle = {{PMLR}},
	author = {Csiba, Dominik and Qu, Zheng and Richtarik, Peter},
	month = jun,
	year = {2015},
	pages = {674--683},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/UX9K5CMW/Csiba et al. - 2015 - Stochastic Dual Coordinate Ascent with Adaptive Pr.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/5RIKRIEF/csiba15.html:text/html}
}

@inproceedings{shalev-shwartz_sdca_2016,
	title = {{SDCA} without {Duality}, {Regularization}, and {Individual} {Convexity}},
	url = {http://proceedings.mlr.press/v48/shalev-shwartza16.html},
	abstract = {Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularizati...},
	language = {en},
	urldate = {2017-07-27},
	booktitle = {{PMLR}},
	author = {Shalev-Shwartz, Shai},
	month = jun,
	year = {2016},
	pages = {747--754},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/WPDNWX5I/Shalev-Shwartz - 2016 - SDCA without Duality, Regularization, and Individu.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/AGDJGW2A/shalev-shwartza16.html:text/html}
}

@article{nesterov_efficiency_2012,
	title = {Efficiency of coordinate descent methods on huge-scale optimization problems},
	volume = {22},
	url = {http://epubs.siam.org/doi/abs/10.1137/100802001},
	number = {2},
	journal = {SIAM Journal on Optimization},
	author = {Nesterov, Yurii},
	year = {2012},
	pages = {341--362},
	file = {[PDF] from siam.org:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/S6CU94QN/Nesterov - 2012 - Efficiency of coordinate descent methods on huge-s.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2929H9K2/100802001.html:text/html}
}

@article{shalev-shwartz_online_2007,
	title = {Online {Learning}: {Theory}, {Algorithms}, and {Applications}},
	url = {http://w3.cs.huji.ac.il/~shais/papers/ShalevThesis07.pdf},
	urldate = {2017-07-28},
	author = {Shalev-Shwartz, Shai},
	year = {2007},
	file = {ShalevThesis07.pdf:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2VKVBC4K/ShalevThesis07.pdf:application/pdf}
}

@article{muller_pystruct_2014,
	title = {pystruct - {Learning} {Structured} {Prediction} in {Python}},
	volume = {15},
	url = {http://www.jmlr.org/papers/v15/mueller14a.html},
	urldate = {2017-07-31},
	journal = {Journal of Machine Learning Research},
	author = {Müller, Andreas C. and Behnke, Sven},
	year = {2014},
	pages = {2055--2060},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/JUJ8SEJW/Müller and Behnke - 2014 - pystruct - Learning Structured Prediction in Pytho.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/T4PWIT3T/mueller14a.html:text/html}
}

@article{lacoste-julien_block-coordinate_2012,
	title = {Block-{Coordinate} {Frank}-{Wolfe} {Optimization} for {Structural} {SVMs}},
	url = {http://arxiv.org/abs/1207.4747},
	abstract = {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.},
	journal = {arXiv:1207.4747 [cs, math, stat]},
	author = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.4747},
	keywords = {90C52, 90C90, 90C06, 68T05, Computer Science - Learning, G.1.6, I.2.6, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv\:1207.4747 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/PJNSV8CJ/Lacoste-Julien et al. - 2012 - Block-Coordinate Frank-Wolfe Optimization for Stru.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/4HIMVM6I/1207.html:text/html}
}

@inproceedings{osokin_minding_2016,
	title = {Minding the {Gaps} for {Block} {Frank}-{Wolfe} {Optimization} of {Structured} {SVMs}},
	url = {http://proceedings.mlr.press/v48/osokin16.html},
	abstract = {In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine...},
	language = {en},
	urldate = {2017-08-02},
	booktitle = {{PMLR}},
	author = {Osokin, Anton and Alayrac, Jean-Baptiste and Lukasewitz, Isabella and Dokania, Puneet and Lacoste-Julien, Simon},
	month = jun,
	year = {2016},
	pages = {593--602},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/KX8TF4HT/Osokin et al. - 2016 - Minding the Gaps for Block Frank-Wolfe Optimizatio.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/9ASHI44D/osokin16.html:text/html}
}

@article{schmidt_minimizing_2013,
	title = {Minimizing {Finite} {Sums} with the {Stochastic} {Average} {Gradient}},
	url = {http://arxiv.org/abs/1309.2388},
	abstract = {We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k{\textasciicircum}\{1/2\}) to O(1/k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1/k) to a linear convergence rate of the form O(p{\textasciicircum}k) for p {\textbackslash}textless\{\} 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies.},
	journal = {arXiv:1309.2388 [cs, math, stat]},
	author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.2388},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1309.2388 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/6TVFXQIB/Schmidt et al. - 2013 - Minimizing Finite Sums with the Stochastic Average.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2XRIIHTV/1309.html:text/html}
}

@book{press_numerical_1992,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Numerical recipes in {C}: the art of scientific computing},
	isbn = {978-0-521-43108-8 978-0-521-43720-2},
	shorttitle = {Numerical recipes in {C}},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	year = {1992},
	keywords = {C (Computer program language)},
	file = {Numerical_Recipes.pdf:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/HUDG92EK/Numerical_Recipes.pdf:application/pdf}
}