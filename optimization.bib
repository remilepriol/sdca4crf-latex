
@article{wilson_lyapunov_2016,
	title = {A {Lyapunov} {Analysis} of {Momentum} {Methods} in {Optimization}},
	url = {http://arxiv.org/abs/1611.02635},
	abstract = {Momentum methods play a central role in optimization. Several momentum methods are provably optimal, and all use a technique called estimate sequences to analyze their convergence properties. The technique of estimate sequences has long been considered difficult to understand, leading many researchers to generate alternative, "more intuitive" methods and analyses. In this paper we show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This framework allows us to develop a simple and unified analysis of many existing momentum algorithms, introduce several new algorithms, and most importantly, strengthen the connection between algorithms and continuous-time dynamical systems.},
	journal = {arXiv:1611.02635 [cs, math]},
	author = {Wilson, Ashia C. and Recht, Benjamin and Jordan, Michael I.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02635},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Optimization and Control},
	file = {arXiv\:1611.02635 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/43KAQH7X/Wilson et al. - 2016 - A Lyapunov Analysis of Momentum Methods in Optimiz.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/P8J6C4RX/1611.html:text/html}
}

@book{kakade_duality_2009,
	title = {On the duality of strong convexity and strong smoothness: {Learning} applications and matrix regularization},
	shorttitle = {On the duality of strong convexity and strong smoothness},
	abstract = {We show that a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. Utilizing this duality, we isolate a single inequality which seamlessly implies both generalization bounds and online regret bounds; and we show how to construct strongly convex functions over matrices based on strongly convex functions over vectors. The newly constructed functions (over matrices) inherit the strong convexity properties of the underlying vector functions. We demonstrate the potential of this framework by analyzing several learning algorithms including group Lasso, kernel learning, and online control with adversarial quadratic costs. 1},
	author = {Kakade, Sham M. and Shalev-shwartz, Shai and Tewari, Ambuj},
	year = {2009},
	file = {Citeseer - Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/5BQ7977A/Kakade et al. - On the duality of strong convexity and strong smoo.pdf:application/pdf;Citeseer - Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/83TMZT3P/summary.html:text/html}
}

@incollection{shalev-shwartz_accelerated_2013,
	title = {Accelerated {Mini}-{Batch} {Stochastic} {Dual} {Coordinate} {Ascent}},
	url = {http://papers.nips.cc/paper/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {378--385},
	file = {NIPS Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/XWD5WRR5/Shalev-Shwartz and Zhang - 2013 - Accelerated Mini-Batch Stochastic Dual Coordinate .pdf:application/pdf;NIPS Snapshort:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/M2CWVRA2/4938-accelerated-mini-batch-stochastic-dual-coordinate-ascent.html:text/html}
}

@article{shalev-shwartz_stochastic_2013,
	title = {Stochastic {Dual} {Coordinate} {Ascent} {Methods} for {Regularized} {Loss} {Minimization}},
	volume = {14},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v14/shalev-shwartz13a.html},
	number = {Feb},
	urldate = {2017-07-06},
	journal = {Journal of Machine Learning Research},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	year = {2013},
	pages = {567--599},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/RC7UTQAC/Shalev-Shwartz and Zhang - 2013 - Stochastic Dual Coordinate Ascent Methods for Regu.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/MEQA7X78/shalev-shwartz13a.html:text/html}
}

@inproceedings{shalev-shwartz_accelerated_2014,
	title = {Accelerated {Proximal} {Stochastic} {Dual} {Coordinate} {Ascent} for {Regularized} {Loss} {Minimization}},
	url = {http://proceedings.mlr.press/v32/shalev-shwartz14.html},
	abstract = {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framewo...},
	language = {en},
	urldate = {2017-07-06},
	booktitle = {{PMLR}},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = jan,
	year = {2014},
	pages = {64--72},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/PSNNVXKH/Shalev-Shwartz and Zhang - 2014 - Accelerated Proximal Stochastic Dual Coordinate As.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/3V5KQBBE/shalev-shwartz14.html:text/html}
}

@article{shalev-shwartz_accelerated_2013-1,
	title = {Accelerated {Proximal} {Stochastic} {Dual} {Coordinate} {Ascent} for {Regularized} {Loss} {Minimization}},
	url = {http://arxiv.org/abs/1309.2375},
	abstract = {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.},
	journal = {arXiv:1309.2375 [cs, stat]},
	author = {Shalev-Shwartz, Shai and Zhang, Tong},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.2375},
	keywords = {Computer Science - Learning, Computer Science - Numerical Analysis, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1309.2375 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/CRWBN39H/Shalev-Shwartz and Zhang - 2013 - Accelerated Proximal Stochastic Dual Coordinate As.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/MZE5MRKF/1309.html:text/html}
}

@inproceedings{csiba_stochastic_2015,
	title = {Stochastic {Dual} {Coordinate} {Ascent} with {Adaptive} {Probabilities}},
	url = {http://proceedings.mlr.press/v37/csiba15.html},
	abstract = {This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowin...},
	language = {en},
	urldate = {2017-07-27},
	booktitle = {{PMLR}},
	author = {Csiba, Dominik and Qu, Zheng and Richtarik, Peter},
	month = jun,
	year = {2015},
	pages = {674--683},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/UX9K5CMW/Csiba et al. - 2015 - Stochastic Dual Coordinate Ascent with Adaptive Pr.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/5RIKRIEF/csiba15.html:text/html}
}

@inproceedings{shalev-shwartz_sdca_2016,
	title = {{SDCA} without {Duality}, {Regularization}, and {Individual} {Convexity}},
	url = {http://proceedings.mlr.press/v48/shalev-shwartza16.html},
	abstract = {Stochastic Dual Coordinate Ascent is a popular method for solving regularized loss minimization for the case of convex losses. We describe variants of SDCA that do not require explicit regularizati...},
	language = {en},
	urldate = {2017-07-27},
	booktitle = {{PMLR}},
	author = {Shalev-Shwartz, Shai},
	month = jun,
	year = {2016},
	pages = {747--754},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/WPDNWX5I/Shalev-Shwartz - 2016 - SDCA without Duality, Regularization, and Individu.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/AGDJGW2A/shalev-shwartza16.html:text/html}
}

@article{nesterov_efficiency_2012,
	title = {Efficiency of coordinate descent methods on huge-scale optimization problems},
	volume = {22},
	url = {http://epubs.siam.org/doi/abs/10.1137/100802001},
	number = {2},
	journal = {SIAM Journal on Optimization},
	author = {Nesterov, Yurii},
	year = {2012},
	pages = {341--362},
	file = {[PDF] from siam.org:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/S6CU94QN/Nesterov - 2012 - Efficiency of coordinate descent methods on huge-s.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2929H9K2/100802001.html:text/html}
}

@article{shalev-shwartz_online_2007,
	title = {Online {Learning}: {Theory}, {Algorithms}, and {Applications}},
	url = {http://w3.cs.huji.ac.il/~shais/papers/ShalevThesis07.pdf},
	urldate = {2017-07-28},
	author = {Shalev-Shwartz, Shai},
	year = {2007},
	file = {ShalevThesis07.pdf:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2VKVBC4K/ShalevThesis07.pdf:application/pdf}
}

@article{muller_pystruct_2014,
	title = {pystruct - {Learning} {Structured} {Prediction} in {Python}},
	volume = {15},
	url = {http://www.jmlr.org/papers/v15/mueller14a.html},
	urldate = {2017-07-31},
	journal = {Journal of Machine Learning Research},
	author = {Müller, Andreas C. and Behnke, Sven},
	year = {2014},
	pages = {2055--2060},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/JUJ8SEJW/Müller and Behnke - 2014 - pystruct - Learning Structured Prediction in Pytho.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/T4PWIT3T/mueller14a.html:text/html}
}

@article{lacoste-julien_block-coordinate_2012,
	title = {Block-{Coordinate} {Frank}-{Wolfe} {Optimization} for {Structural} {SVMs}},
	url = {http://arxiv.org/abs/1207.4747},
	abstract = {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.},
	journal = {arXiv:1207.4747 [cs, math, stat]},
	author = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.4747},
	keywords = {90C52, 90C90, 90C06, 68T05, Computer Science - Learning, G.1.6, I.2.6, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv\:1207.4747 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/PJNSV8CJ/Lacoste-Julien et al. - 2012 - Block-Coordinate Frank-Wolfe Optimization for Stru.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/4HIMVM6I/1207.html:text/html}
}

@inproceedings{osokin_minding_2016,
	title = {Minding the {Gaps} for {Block} {Frank}-{Wolfe} {Optimization} of {Structured} {SVMs}},
	url = {http://proceedings.mlr.press/v48/osokin16.html},
	abstract = {In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine...},
	language = {en},
	urldate = {2017-08-02},
	booktitle = {{PMLR}},
	author = {Osokin, Anton and Alayrac, Jean-Baptiste and Lukasewitz, Isabella and Dokania, Puneet and Lacoste-Julien, Simon},
	month = jun,
	year = {2016},
	pages = {593--602},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/KX8TF4HT/Osokin et al. - 2016 - Minding the Gaps for Block Frank-Wolfe Optimizatio.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/9ASHI44D/osokin16.html:text/html}
}

@article{schmidt_minimizing_2015,
	title = {Minimizing {Finite} {Sums} with the {Stochastic} {Average} {Gradient}},
	url = {http://arxiv.org/abs/1309.2388},
	abstract = {We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k{\textasciicircum}\{1/2\}) to O(1/k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1/k) to a linear convergence rate of the form O(p{\textasciicircum}k) for p {\textbackslash}textless\{\} 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies.},
	journal = {arXiv:1309.2388 [cs, math, stat]},
	author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
	month = jan,
	year = {2015},
	note = {arXiv: 1309.2388},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1309.2388 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/6TVFXQIB/Schmidt et al. - 2013 - Minimizing Finite Sums with the Stochastic Average.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/2XRIIHTV/1309.html:text/html}
}

@book{press_numerical_1992,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Numerical recipes in {C}: the art of scientific computing},
	isbn = {978-0-521-43108-8 978-0-521-43720-2},
	shorttitle = {Numerical recipes in {C}},
	publisher = {Cambridge University Press},
	author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
	year = {1992},
	keywords = {C (Computer program language)},
	file = {Numerical_Recipes.pdf:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/HUDG92EK/Numerical_Recipes.pdf:application/pdf}
}

@article{dziugaite_computing_2017,
	title = {Computing {Nonvacuous} {Generalization} {Bounds} for {Deep} ({Stochastic}) {Neural} {Networks} with {Many} {More} {Parameters} than {Training} {Data}},
	url = {http://arxiv.org/abs/1703.11008},
	abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous in this "deep learning" regime. In order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
	journal = {arXiv:1703.11008 [cs]},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.11008},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1703.11008 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/28H3KVZP/Dziugaite and Roy - 2017 - Computing Nonvacuous Generalization Bounds for Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/F74D7BV6/1703.html:text/html}
}

@article{collins_exponentiated_2008,
	title = {Exponentiated {Gradient} {Algorithms} for {Conditional} {Random} {Fields} and {Max}-{Margin} {Markov} {Networks}},
	volume = {9},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v9/collins08a.html},
	number = {Aug},
	urldate = {2017-08-29},
	journal = {Journal of Machine Learning Research},
	author = {Collins, Michael and Globerson, Amir and Koo, Terry and Carreras, Xavier and Bartlett, Peter L.},
	year = {2008},
	pages = {1775--1822},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/H76TNE8P/Collins et al. - 2008 - Exponentiated Gradient Algorithms for Conditional .pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/V5KERKS6/collins08a.html:text/html}
}

@inproceedings{zhao_stochastic_2015,
	title = {Stochastic {Optimization} with {Importance} {Sampling} for {Regularized} {Loss} {Minimization}},
	url = {http://proceedings.mlr.press/v37/zhaoa15.html},
	abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordin...},
	language = {en},
	urldate = {2017-08-29},
	booktitle = {{PMLR}},
	author = {Zhao, Peilin and Zhang, Tong},
	month = jun,
	year = {2015},
	pages = {1--9},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/VX3T7AFQ/Zhao and Zhang - 2015 - Stochastic Optimization with Importance Sampling f.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/AD3CE44P/zhaoa15.html:text/html}
}

@article{schmidt_non-uniform_2015,
	title = {Non-{Uniform} {Stochastic} {Average} {Gradient} {Method} for {Training} {Conditional} {Random} {Fields}},
	url = {http://arxiv.org/abs/1504.04406},
	abstract = {We apply stochastic average gradient (SAG) algorithms for training conditional random fields (CRFs). We describe a practical implementation that uses structure in the CRF gradient to reduce the memory requirement of this linearly-convergent stochastic gradient method, propose a non-uniform sampling scheme that substantially improves practical performance, and analyze the rate of convergence of the SAGA variant under non-uniform sampling. Our experimental results reveal that our method often significantly outperforms existing methods in terms of the training objective, and performs as well or better than optimally-tuned stochastic gradient methods in terms of test error.},
	journal = {arXiv:1504.04406 [cs, math, stat]},
	author = {Schmidt, Mark and Babanezhad, Reza and Ahmed, Mohamed Osama and Defazio, Aaron and Clifton, Ann and Sarkar, Anoop},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.04406},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1504.04406 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/KRKT8CQT/Schmidt et al. - 2015 - Non-Uniform Stochastic Average Gradient Method for.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/C8FNRWQB/1504.html:text/html}
}

@article{taskar_structured_2006,
	title = {Structured {Prediction}, {Dual} {Extragradient} and {Bregman} {Projections}},
	volume = {7},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v7/taskar06a.html},
	number = {Jul},
	urldate = {2017-09-11},
	journal = {Journal of Machine Learning Research},
	author = {Taskar, Ben and Lacoste-Julien, Simon and Jordan, Michael I.},
	year = {2006},
	pages = {1627--1653},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/JRUDXTCX/Taskar et al. - 2006 - Structured Prediction, Dual Extragradient and Breg.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/7N2AX5T2/taskar06a.html:text/html}
}

@article{defazio_saga:_2014,
	title = {{SAGA}: {A} {Fast} {Incremental} {Gradient} {Method} {With} {Support} for {Non}-{Strongly} {Convex} {Composite} {Objectives}},
	shorttitle = {{SAGA}},
	url = {http://arxiv.org/abs/1407.0202},
	abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
	journal = {arXiv:1407.0202 [cs, math, stat]},
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.0202},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv\:1407.0202 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/GK4NWSXP/Defazio et al. - 2014 - SAGA A Fast Incremental Gradient Method With Supp.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/TFSNZH5P/1407.html:text/html}
}

@inproceedings{jaggi_revisiting_2013,
	title = {Revisiting {Frank}-{Wolfe}: {Projection}-{Free} {Sparse} {Convex} {Optimization}},
	shorttitle = {Revisiting {Frank}-{Wolfe}},
	url = {http://proceedings.mlr.press/v28/jaggi13.html},
	abstract = {We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework...},
	language = {en},
	urldate = {2017-09-11},
	booktitle = {{PMLR}},
	author = {Jaggi, Martin},
	month = feb,
	year = {2013},
	pages = {427--435},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/3ZF746RM/Jaggi - 2013 - Revisiting Frank-Wolfe Projection-Free Sparse Con.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/VRQGDUQX/jaggi13.html:text/html}
}

@incollection{taskar_max-margin_2004,
	title = {Max-{Margin} {Markov} {Networks}},
	url = {http://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Taskar, Ben and Guestrin, Carlos and Koller, Daphne},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, P. B.},
	year = {2004},
	pages = {25--32},
	file = {NIPS Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/92CUZN6Q/Taskar et al. - 2004 - Max-Margin Markov Networks.pdf:application/pdf;NIPS Snapshort:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/R3NRSFKG/2397-max-margin-markov-networks.html:text/html}
}

@article{lafferty_conditional_2001,
	title = {Conditional {Random} {Fields}: {Probabilistic} {Models} for {Segmenting} and {Labeling} {Sequence} {Data}},
	shorttitle = {Conditional {Random} {Fields}},
	url = {http://repository.upenn.edu/cis_papers/159},
	journal = {Departmental Papers (CIS)},
	author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
	month = jun,
	year = {2001},
	file = {"Conditional Random Fields\: Probabilistic Models for Segmenting and Lab" by John Lafferty, Andrew McCallum et al.:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/R594P2KH/159.html:text/html}
}

@book{wallach_efficient_2002,
	title = {Efficient {Training} of {Conditional} {Random} {Fields}},
	abstract = {This thesis explores a number of parameter estimation techniques for conditional random fields, a recently introduced probabilistic model for labelling and segmenting sequential data. Theoretical and practical disadvantages of the training techniques reported in current literature on CRFs are discussed. We hypothesise that general numerical optimisation techniques result in improved performance over iterative scaling algorithms for training CRFs. Experiments run on a a subset of a well-known text chunking data set confirm that this is indeed the case. This is a highly promising result, indicating that such parameter estimation techniques make CRFs a practical and efficient choice for labelling sequential data, as well as a theoretically sound and principled probabilistic framework.},
	author = {Wallach, Hanna},
	year = {2002},
	file = {Citeseer - Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/4RRII4E5/summary.html:text/html}
}

@inproceedings{sha_shallow_2003,
	address = {Stroudsburg, PA, USA},
	series = {{NAACL} '03},
	title = {Shallow {Parsing} with {Conditional} {Random} {Fields}},
	url = {https://doi.org/10.3115/1073445.1073473},
	doi = {10.3115/1073445.1073473},
	abstract = {Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.},
	booktitle = {Proceedings of the 2003 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics} on {Human} {Language} {Technology} - {Volume} 1},
	publisher = {Association for Computational Linguistics},
	author = {Sha, Fei and Pereira, Fernando},
	year = {2003},
	pages = {134--141},
	file = {ACM Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/VFGS5VBH/Sha and Pereira - 2003 - Shallow Parsing with Conditional Random Fields.pdf:application/pdf}
}

@inproceedings{vishwanathan_accelerated_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Accelerated {Training} of {Conditional} {Random} {Fields} with {Stochastic} {Gradient} {Methods}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143966},
	doi = {10.1145/1143844.1143966},
	abstract = {We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Vishwanathan, S. V. N. and Schraudolph, Nicol N. and Schmidt, Mark W. and Murphy, Kevin P.},
	year = {2006},
	pages = {969--976}
}

@inproceedings{finkel_efficient_2008,
	title = {Efficient, {Feature}-based, {Conditional} {Random} {Field} {Parsing}},
	url = {http://aclasb.dfki.de/nlp/bib/P08-1109},
	urldate = {2017-09-14},
	publisher = {Association for Computational Linguistics},
	author = {Finkel, Jenny Rose and Kleeman, Alex and Manning, Christopher D.},
	year = {2008},
	pages = {959--967},
	file = {Full Text PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/4J4SNNJM/Finkel et al. - 2008 - Efficient, Feature-based, Conditional Random Field.pdf:application/pdf;Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/UNGCNWAW/P08-1109.html:text/html}
}

@book{noauthor_introductory_nodate,
	title = {Introductory {Lectures} on {Convex} {Optimization} - {A} {Basic} {Course} {\textbar} {Yurii} {Nesterov} {\textbar} {Springer}},
	url = {http://www.springer.com/gp/book/9781402075537},
	abstract = {It was in the middle of the 1980s, when the seminal paper by Kar­ markar opened a new epoch in nonlinear optimization. The importance of this paper,...},
	urldate = {2017-09-14},
	file = {Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/EIHWKFP8/9781402075537.html:text/html}
}

@article{roux_stochastic_2012,
	title = {A {Stochastic} {Gradient} {Method} with an {Exponential} {Convergence} {Rate} for {Finite} {Training} {Sets}},
	url = {http://arxiv.org/abs/1202.6258},
	abstract = {We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.},
	journal = {arXiv:1202.6258 [cs, math]},
	author = {Roux, Nicolas Le and Schmidt, Mark and Bach, Francis},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.6258},
	keywords = {Computer Science - Learning, Mathematics - Optimization and Control},
	file = {arXiv\:1202.6258 PDF:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/NWKM3J6A/Roux et al. - 2012 - A Stochastic Gradient Method with an Exponential C.pdf:application/pdf;arXiv.org Snapshot:/Users/remilepriol/Library/Application Support/Zotero/Profiles/fbxcz3st.default/zotero/storage/IZW6MQFQ/1202.html:text/html}
}